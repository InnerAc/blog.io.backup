title: 在java程序中将spark的RDD缓存到tachyon中
date: 2016-01-01 16:48:59
categories: 编程与算法
tags: [spark,tachyon,大数据,java]
---
之前做好了可以在`spark`中读取`tachyon`的配置，但是使用`rdd.persist(StorageLevel.OFF_HEAP)`就出错，经过不断的排错和寻找，终于解决了这个问题。并且也可以在`java`中去设置了，之前设置一直出错。

<!--more-->

## 1 spark的路径配置
其实`spark`这个配置文件之前一直忘记配置：就是在`spark/conf`下的`spark-defaults.conf.template`首先执行:

```shell
cp spark-defaults.conf.template spark-defaults.conf
```
然后在里面加入:
```conf
spark.tachyonStore.url	tachyon://master:19998
```
其实还有一个属性`park.tachyonStore.baseDir`可以不加的，如果不加使用的是默认位置，其实是我加上之后就会报错，所以我才不加。

加好之后重启，然后运行`spark-shell`。
执行下述步骤:
```scala
scala> import org.apache.spark.storage.StorageLevel
scala> val rdd = sc.textFile("tachyon://master:19998/hhu/input/f2")
scala> rdd.persist(StorageLevel.OFF_HEAP)
scala> rdd.count()
```
执行成功即可表示配置完成，可以在`master:19999`文件浏览查看到临时创建的目录，但是已经没有东西了，因为运行结束之后就会删除。

## 2 在java中使用

在`scala`中使用的好方便，但是`java`中总是出错。而且又没几个人用`java`去写。所以之后自己接着查`api`，最后还是搞定了。

开始要在`hadoop`配置里面加上`"fs.tachyon.impl", "tachyon.hadoop.TFS"`。

然后就是获取到`RDD`后，要先执行`rdd.unpersist();`然后才能重新设置。否则会报错。例子贴一下：
```java
package edu.hhu.innerac.sparktest;  

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.storage.StorageLevel;
 
public class Test {
 
    static final String USER = "innerac";
 
    public static void main(String[] args) throws Exception {

    	SparkConf conf = new SparkConf();
    	
        JavaSparkContext sc = new JavaSparkContext("spark://master:7077", "Spark App 0", conf);
        
        sc.hadoopConfiguration().set("fs.tachyon.impl", "tachyon.hadoop.TFS");
//        String file = "hdfs://master:9000/hhu/input/f2";
        String file = "tachyon://master:19998/hhu/input/f2";
        
        JavaRDD<String> rdd = null;
        
        rdd = sc.textFile(file, 4).cache();
        
        rdd.unpersist();
        rdd.persist(StorageLevel.OFF_HEAP());
        System.out.println(rdd.count());
        
    }
 
}
```