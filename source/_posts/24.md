title: 一个用python写的科学上网小脚本（上）
categories: 编程与算法
tags: [python,ss]
date: 2015-08-25 08:42:47
---
之前写过一篇科学上网的文章。关于什么是科学上网，大家都懂的。详情请见：[点我传送][1]
但是科学上网要有ss账号啊，咱还是个穷学生，舍不得买vpn。好吧较便宜的ss账号也舍不得买。还要很多网站都提供免费的ss账号。这可是良心啊。当然啦，网站神马的就不推荐了，想用的话百度之。


<!--more-->

我们就来说说，如何不用打开网站就可完成一键获取ss账号并且完成ss连接。
大神勿喷，这都是我自己想的，所以肯定有大神们看不上的地方，欢迎指出不足，谢谢。(毕竟刚刚接触python)
简单来说一下实现的步骤：

 1. 抓取网页
 2. 解析网页
 3. 得到服务器ip、端口、密码、加密方式等。
 4. 生成json文件
 5. 执行科学上网命令(见之前的文章 [点我传送][2])
## 抓取网页 ##
首先，我们应该抓取网页。这里我使用的是python的urllib2库。据说这个库很强大。
通过这个库，使用get的方式请求了html。大部分都写了个这个函数
```python
def getHtml(url):
    page = urllib2.urlopen(url)
    html = page.read()
    return html
```
然而，当我去请求目标url的时候，并没有得到访问结果。我猜，估计是没有加浏览器头的原因吧。这真是猜的，因为好多网站防爬虫什么的会进行个判断。那我只能改进一下这个函数了，把user-agent加进去。如下：
```python
def getHtml(url):
    req = urllib2.Request(url)
    req.add_header('User-Agent','Mozilla/5.0 (Windows NT 6.2; rv:16.0) Gecko/20100101 Firefox/16.0')
    page = urllib2.urlopen(req)
    html = page.read()
    return html
```
再次运行脚本，成功得到html。
这是要用到的部分：
![aaddress.png][3]
## 解析网页 ##
本来想直接用正则去匹配就结束了，可是谁知道为毛成功不了。所以就用了一个看起来很吊的一个库`BeautifulSoup`，使用这个库时要先安装。
ubuntu用户直接执行`sudo apt-get install Python-bs4`即可。其他发行版的话，自行百度吧。
导入的话就是这个咯`from bs4 import BeautifulSoup`
然后将得到的html转换为beautifulsoup格式。同时找到相应的div
```python
soup = BeautifulSoup(html)
div = soup.find('div',{'class':"col-lg-4 text-center"})
hfs = div.findAll('h4')
```
`find`就是找到第一个，`findAll`就是找到所有的然后返回一个list
这样我们就得到包含所有想要信息的list啦。
`hfs[i].string`就可以得到想要的字符串了。
不过它的字符串还包括前面的说明，所以要通过正则过滤掉。
正则如下：
`regexAll = r'\:(.*)'`就是把`:`后面的字符留下啦。
比如：
`port = re.search(regexAll,hfs[1].string).group(1)`
就可以得到它的端口u'8989'
因为是一个unicode所以会在前面加`u`，这个转换是我百度的，如下：
`string.atoi(eval(repr(port)[1:]))`
前面是用来转换为数字的，括号内是把`u`去掉的。
现在我们已经得到所有该得到的信息了。
后面两个先留着吧，困死了，睡午觉去。

  [1]: http://www.eternalac.com/index.php/archives/7/
  [2]: http://www.eternalac.com/index.php/archives/7/
  [3]: http://www.eternalac.com/usr/uploads/2015/08/879485559.png